---
phase: 04-validation
plan: 06
type: execute
wave: 3
depends_on: ["04-01", "04-02", "04-03", "04-04", "04-05"]
files_modified:
  - core/validation/__init__.py
  - core/validation/orchestrator.py
  - apps/cli/cli.py
  - tests/unit/test_validation.py
  - tests/integration/test_validation_e2e.py
autonomous: true

must_haves:
  truths:
    - "User can run 'gsd validate' to detect all continuity issues"
    - "ValidationOrchestrator runs all validators and aggregates results"
    - "Issues persist to build/issues.json with deterministic output"
    - "Reports generated in vault/80_Reports/"
    - "CLI returns non-zero exit code on errors"
  artifacts:
    - path: "core/validation/orchestrator.py"
      provides: "ValidationOrchestrator coordinating all validators"
      min_lines: 80
    - path: "apps/cli/cli.py"
      provides: "'gsd validate' command"
      contains: "cmd_validate"
    - path: "tests/unit/test_validation.py"
      provides: "Unit tests for validators"
      min_lines: 100
    - path: "tests/integration/test_validation_e2e.py"
      provides: "End-to-end validation tests"
      min_lines: 80
  key_links:
    - from: "cmd_validate()"
      to: "ValidationOrchestrator"
      via: "import and instantiate"
      pattern: "from core.validation import ValidationOrchestrator"
    - from: "ValidationOrchestrator.run_validation()"
      to: "build/issues.json"
      via: "_save_issues()"
      pattern: "issues_path.write_text"
---

<objective>
Integrate validation into CLI with ValidationOrchestrator and add comprehensive tests.

Purpose: Enable `gsd validate` command that runs all validators, saves issues, generates reports, and provides clear feedback.
Output: CLI command, orchestrator, and test suite
</objective>

<execution_context>
@/Users/bretbouchard/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bretbouchard/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/REQUIREMENTS.md
@.planning/phases/04-validation/04-01-SUMMARY.md
@.planning/phases/04-validation/04-02-SUMMARY.md
@.planning/phases/04-validation/04-03-SUMMARY.md
@.planning/phases/04-validation/04-04-SUMMARY.md
@.planning/phases/04-validation/04-05-SUMMARY.md

# Reference CLI patterns
@apps/cli/cli.py

# Reference all validators
@core/validation/__init__.py
@core/validation/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ValidationOrchestrator</name>
  <files>core/validation/orchestrator.py, core/validation/__init__.py</files>
  <action>
Create ValidationOrchestrator class that coordinates all validators:

1. **ValidationOrchestrator class**:
   - __init__(self, project_path: Path)
   - self.build_path = project_path / "build"
   - self.vault_path = project_path / "vault"
   - Initialize all validators: WardrobeValidator, PropsValidator, TimelineValidator, KnowledgeValidator
   - Initialize ReportGenerator

2. **run_validation() -> Dict[str, Any]**:
   - Check prerequisites: storygraph.json must exist
   - Run each validator's validate() method
   - Collect all issues
   - Re-number issue IDs globally (issue_000001, issue_000002, etc.)
   - Call _save_issues()
   - Call report_generator.generate_reports()
   - Return summary dict:
     ```python
     {
         "success": error_count == 0,
         "total_issues": int,
         "by_severity": {"error": int, "warning": int, "info": int},
         "by_category": {"wardrobe": int, "props": int, "timeline": int, "knowledge": int},
         "reports": {"summary": str, ...},
         "issues_path": str
     }
     ```

3. **_save_issues()**:
   - Create build/issues.json with structure:
     ```json
     {
       "version": "1.0",
       "generated_at": "ISO timestamp",
       "total_issues": int,
       "summary": {...},
       "issues": [sorted by severity then scene_number]
     }
     ```
   - Sort issues deterministically (severity, scene_number, issue_id)
   - Use json.dumps with sort_keys=True

4. **_get_summary() -> Dict[str, Any]**:
   - Aggregate counts by severity, category
   - Include auto_fixable count

5. **Update core/validation/__init__.py**:
   - Export ValidationOrchestrator
   - Create convenience function: validate_project(project_path: Path) -> Dict[str, Any]
</action>
  <verify>python -c "from core.validation import ValidationOrchestrator; from pathlib import Path; vo = ValidationOrchestrator(Path('.')); print('OK')" </verify>
  <done>ValidationOrchestrator runs all validators, saves issues.json, generates reports, returns summary dict</done>
</task>

<task type="auto">
  <name>Task 2: Add CLI validate command</name>
  <files>apps/cli/cli.py</files>
  <action>
Add `gsd validate` command to CLI following existing patterns:

1. **cmd_validate(args: argparse.Namespace) -> int**:
   - Find project root (use find_project_root() pattern)
   - Load config
   - Check prerequisites: build/storygraph.json must exist
   - Print "Running validation..." header
   - Import and create ValidationOrchestrator
   - Call run_validation()
   - Print results:
     ```
     === Validation Results ===
     Errors:   X
     Warnings: Y
     Info:     Z

     Reports generated in vault/80_Reports/
       Summary: vault/80_Reports/validation-summary.md
     ```
   - Return 0 if no errors, 1 if errors found (useful for CI)

2. **Add subparser in main()**:
   - p_validate = subparsers.add_parser("validate", help="Validate story continuity")
   - p_validate.set_defaults(func=cmd_validate)

3. **Error handling**:
   - If not in project, print error and return 1
   - If no storygraph.json, print "Run 'gsd build canon' first" and return 1
   - Catch and report any validator exceptions

Follow the pattern of cmd_sync() for structure and error handling.
</action>
  <verify>cd /Users/bretbouchard/apps/fdx_gsd && python -c "from apps.cli.cli import cmd_validate; print('cmd_validate exists')" && python -m apps.cli.cli --help | grep validate</verify>
  <done>gsd validate command works, runs all validators, prints summary, returns non-zero on errors</done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for validators</name>
  <files>tests/unit/test_validation.py</files>
  <action>
Create comprehensive unit tests following existing test patterns:

1. **Test fixtures** in tests/fixtures/validation/:
   - Create minimal storygraph.json with test entities:
     - 2 characters with wardrobe mentions
     - 2 locations with distance
     - 3 scenes with time markers
   - Create minimal scriptgraph.json if needed

2. **TestIssueDataModel** class:
   - test_issue_to_dict()
   - test_issue_from_dict()
   - test_issue_severity_ordering()
   - test_issue_id_generation()

3. **TestWardrobeValidator** class:
   - test_no_issues_with_consistent_wardrobe()
   - test_wardrobe_change_without_cause() - WARD-01
   - test_wardrobe_change_with_time_skip() - no issue
   - test_wardrobe_conflict_in_continuous_time() - WARD-02
   - test_missing_signature_item() - WARD-03

4. **TestPropsValidator** class:
   - test_no_issues_with_proper_prop_introduction() - no issue
   - test_prop_appears_without_introduction() - PROP-01
   - test_prop_ownership_transfer_shown() - no issue
   - test_prop_ownership_transfer_missing() - PROP-02
   - test_prop_damage_persists() - PROP-03

5. **TestTimelineValidator** class:
   - test_no_issues_with_realistic_travel() - no issue
   - test_impossible_travel() - TIME-01
   - test_resolved_time_phrase() - no issue
   - test_unresolved_time_phrase() - TIME-02
   - test_character_in_two_places() - TIME-04

6. **TestKnowledgeValidator** class:
   - test_knowledge_acquired_before_use() - no issue
   - test_knowledge_used_before_learning() - KNOW-01
   - test_secret_propagates_through_shown_channel() - no issue
   - test_secret_propagates_without_channel() - KNOW-02
   - test_motive_consistency() - KNOW-03
   - test_relationship_continuity() - KNOW-04

Use pytest patterns from existing tests. Each test should be self-contained with its own fixture data.
</action>
  <verify>cd /Users/bretbouchard/apps/fdx_gsd && python -m pytest tests/unit/test_validation.py -v --tb=short 2>&1 | head -50</verify>
  <done>Unit tests pass for Issue data model, all 4 validators with positive and negative cases</done>
</task>

<task type="auto">
  <name>Task 4: Create integration tests for validation pipeline</name>
  <files>tests/integration/test_validation_e2e.py</files>
  <action>
Create end-to-end integration tests:

1. **TestValidationPipeline** class:
   - Uses full project fixture with complete storygraph and scriptgraph
   - Tests the full validation flow from CLI to reports

2. **test_full_validation_pipeline()**:
   - Create temp project with test fixtures
   - Call ValidationOrchestrator.run_validation()
   - Verify issues.json created
   - Verify reports generated
   - Verify issue counts match expected

3. **test_validate_command_integration()**:
   - Use subprocess to run `gsd validate` on test project
   - Verify exit code (0 for no errors, 1 for errors)
   - Verify stdout contains expected output

4. **test_report_generation()**:
   - Run validation
   - Read generated markdown reports
   - Verify Obsidian wikilinks format: [[entity_id]]
   - Verify severity icons present
   - Verify grouped by scene

5. **test_deterministic_output()**:
   - Run validation twice
   - Compare issues.json outputs
   - Verify identical (sorted, deterministic)

6. **Test fixtures**:
   - tests/fixtures/validation_project/ with complete structure
   - storygraph.json with known issues
   - Expected issues.json for comparison

Follow integration test patterns from tests/integration/test_roundtrip.py.
</action>
  <verify>cd /Users/bretbouchard/apps/fdx_gsd && python -m pytest tests/integration/test_validation_e2e.py -v --tb=short 2>&1 | head -50</verify>
  <done>Integration tests pass, covering full validation pipeline, CLI command, report generation, determinism</done>
</task>

</tasks>

<verification>
- Run: gsd validate (in a test project with storygraph.json)
- Verify build/issues.json created
- Verify vault/80_Reports/validation-summary.md exists
- Run: python -m pytest tests/unit/test_validation.py tests/integration/test_validation_e2e.py -v
</verification>

<success_criteria>
- `gsd validate` command works
- All validators integrated through ValidationOrchestrator
- Issues persisted to build/issues.json (deterministic)
- Reports generated in vault/80_Reports/ (Obsidian-compatible)
- CLI returns non-zero on errors (CI-friendly)
- Unit tests pass for all validators
- Integration tests cover full pipeline
</success_criteria>

<output>
After completion, create `.planning/phases/04-validation/04-06-SUMMARY.md`
</output>
